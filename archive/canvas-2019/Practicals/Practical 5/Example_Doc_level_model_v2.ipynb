{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doc_level_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4BB42bolutX",
        "colab_type": "code",
        "outputId": "9b60b10a-473e-48e5-d559-ecb4f6ec9f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIFHkDlzmXZI",
        "colab_type": "code",
        "outputId": "7a5309c0-d587-4716-a0de-c8c05737b04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My Drive/Recsys-2019"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Recsys-2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTvH9Q5vygvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "logging.basicConfig(\n",
        "                    # filename='out.log',\n",
        "                    level=logging.INFO,\n",
        "                    format='%(asctime)s %(levelname)s %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U2hpF0Eywvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle  \n",
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def save_we(vocab, weights, domain):\n",
        "    word_emb = {}\n",
        "    for i, j in zip(vocab, weights):\n",
        "        word_emb[i] = j\n",
        "    if domain == 'amazon_electronics':\n",
        "        save_obj(word_emb, 'data/word_emb_lt')\n",
        "    else:\n",
        "        save_obj(word_emb, 'data/word_emb_res')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhiEyxIRm_HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import operator\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
        "\n",
        "def create_vocab(domain, maxlen=0, vocab_size=0):\n",
        "    \n",
        "    print('Creating vocab ...')\n",
        "\n",
        "    f = 'data/doc_level/%s_text.txt'%(domain)\n",
        "\n",
        "    total_words, unique_words = 0, 0\n",
        "    word_freqs = {}\n",
        "\n",
        "    fin = codecs.open(f, 'r', 'utf-8')\n",
        "    for line in fin:\n",
        "        words = line.split()\n",
        "        if maxlen > 0 and len(words) > maxlen:\n",
        "            continue\n",
        "\n",
        "        for w in words:\n",
        "            if not bool(num_regex.match(w)):\n",
        "                try:\n",
        "                    word_freqs[w] += 1\n",
        "                except KeyError:\n",
        "                    unique_words += 1\n",
        "                    word_freqs[w] = 1\n",
        "                total_words += 1\n",
        "\n",
        "    print ('  %i total words, %i unique words' % (total_words, unique_words))\n",
        "    sorted_word_freqs = sorted(word_freqs.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "    vocab = {'<pad>':0, '<unk>':1, '<num>':2}\n",
        "    index = len(vocab)\n",
        "    for word, _ in sorted_word_freqs:\n",
        "        vocab[word] = index\n",
        "        index += 1\n",
        "        if vocab_size > 0 and index > vocab_size + 2:\n",
        "            break\n",
        "    if vocab_size > 0:\n",
        "        print (' keep the top %i words' % vocab_size)\n",
        "\n",
        "    #Write vocab to a txt file\n",
        "    # vocab_file = codecs.open(domain+'_vocab', mode='w', encoding='utf8')\n",
        "    # sorted_vocab = sorted(vocab.items(), key=operator.itemgetter(1))\n",
        "    # for word, index in sorted_vocab:\n",
        "    #     vocab_file.write(word+'\\t'+str(index)+'\\n')\n",
        "    # vocab_file.close()\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def create_data(vocab, text_path, label_path, domain, skip_top, skip_len, replace_non_vocab):\n",
        "    data = []\n",
        "    label = [] # {pos: 0, neg: 1, neu: 2}\n",
        "    f = codecs.open(text_path, 'r', 'utf-8')\n",
        "    f_l = codecs.open(label_path, 'r', 'utf-8')\n",
        "    num_hit, unk_hit, skip_top_hit, total = 0., 0., 0., 0.\n",
        "    pos_count, neg_count, neu_count = 0, 0, 0\n",
        "    max_len = 0\n",
        "\n",
        "    for line, score in zip(f, f_l):\n",
        "        word_indices = []\n",
        "        words = line.split()\n",
        "        if skip_len > 0 and len(words) > skip_len:\n",
        "            continue\n",
        "\n",
        "        score = float(score.strip())\n",
        "        if score < 3:\n",
        "            neg_count += 1\n",
        "            label.append(1)\n",
        "        elif score > 3:\n",
        "            pos_count += 1\n",
        "            label.append(0)\n",
        "        else:\n",
        "            neu_count += 1\n",
        "            label.append(2)\n",
        "          \n",
        "        for word in words:\n",
        "            if bool(num_regex.match(word)):\n",
        "                word_indices.append(vocab['<num>'])\n",
        "                num_hit += 1\n",
        "            elif word in vocab:\n",
        "                word_ind = vocab[word]\n",
        "                if skip_top > 0 and word_ind < skip_top + 3:\n",
        "                    skip_top_hit += 1\n",
        "                else:\n",
        "                    word_indices.append(word_ind)\n",
        "            else:\n",
        "                if replace_non_vocab:\n",
        "                    word_indices.append(vocab['<unk>'])\n",
        "                unk_hit += 1\n",
        "            total += 1\n",
        "\n",
        "        if len(word_indices) > max_len:\n",
        "            max_len = len(word_indices)\n",
        "\n",
        "        data.append(word_indices)\n",
        "\n",
        "    f.close()\n",
        "    f_l.close()\n",
        "\n",
        "    print('  <num> hit rate: %.2f%%, <unk> hit rate: %.2f%%' % (100*num_hit/total, 100*unk_hit/total))\n",
        "\n",
        "    print (domain)\n",
        "    print( 'pos count: ', pos_count )\n",
        "    print( 'neg count: ', neg_count )\n",
        "    print( 'neu count: ', neu_count )\n",
        "\n",
        "    return np.array(data), np.array(label), max_len\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data(domain, vocab_size, skip_top=0, skip_len=0, replace_non_vocab=1):\n",
        "\n",
        "    assert domain in ['amazon_electronics', 'yelp14']\n",
        "\n",
        "    vocab = create_vocab(domain, skip_len, vocab_size)\n",
        "\n",
        "    text_path = 'data/doc_level/%s_text.txt'%(domain)\n",
        "    score_path = 'data/doc_level/%s_label.txt'%(domain)\n",
        "\n",
        "    data, label, max_len = create_data(vocab, text_path, score_path, domain, skip_top, skip_len, replace_non_vocab)\n",
        "\n",
        "    return vocab, data, label, max_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNceF301rnM5",
        "colab_type": "text"
      },
      "source": [
        "### For doc-level/amazon_electronics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMqj1I7MnN2B",
        "colab_type": "code",
        "outputId": "36d10081-7de0-497d-c078-dc20af88e4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "vocab_am, data_list_am, label_list_am, overall_maxlen_am = prepare_data('amazon_electronics', 10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating vocab ...\n",
            "  3440972 total words, 39122 unique words\n",
            " keep the top 10000 words\n",
            "  <num> hit rate: 1.04%, <unk> hit rate: 1.56%\n",
            "amazon_electronics\n",
            "pos count:  10000\n",
            "neg count:  10000\n",
            "neu count:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H-5LbQ8scpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_words_am = dict((v,k) for (k,v) in vocab_am.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ckZQs4wsPUe",
        "colab_type": "text"
      },
      "source": [
        "###  For doc-level/yelp_2014"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp_MkulGr5RZ",
        "colab_type": "code",
        "outputId": "6f69104f-abd2-4517-da1a-b3e2d02ec0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "vocab_yelp, data_list_yelp, label_list_yelp, overall_maxlen_yelp = prepare_data('yelp14', 10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating vocab ...\n",
            "  3829257 total words, 43607 unique words\n",
            " keep the top 10000 words\n",
            "  <num> hit rate: 0.87%, <unk> hit rate: 2.05%\n",
            "yelp14\n",
            "pos count:  10000\n",
            "neg count:  10000\n",
            "neu count:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkAMTb8lt0Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_words_yelp = dict((v,k) for (k,v) in vocab_yelp.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13GNLSaWu7np",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0I4Ob1uvL3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Input\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myrNjPinu9JC",
        "colab_type": "code",
        "outputId": "6999e8e3-9aa9-415e-b34c-d3dbe8007eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "###################################\n",
        "## Create Model\n",
        "#\n",
        "\n",
        "dropout = 0.5    \n",
        "recurrent_dropout = 0.1    \n",
        "#vocab_size = len(vocab_am)\n",
        "vocab_size = len(vocab_yelp)\n",
        "\n",
        "##### Inputs #####\n",
        "sentence_input = Input(shape=(None,), dtype='int32', name='sentence_input')\n",
        "\n",
        "word_emb = Embedding(vocab_size, 300, mask_zero=True, name='word_emb')\n",
        "output = word_emb(sentence_input)\n",
        "\n",
        "print ('use a rnn layer')\n",
        "output = LSTM(300, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout, name='lstm')(output)\n",
        "\n",
        "print ('use 0.5 dropout layer')\n",
        "output = Dropout(0.5)(output)\n",
        "\n",
        "densed = Dense(3, name='dense')(output)\n",
        "probs = Activation('softmax')(densed)\n",
        "model = Model(inputs=[sentence_input], outputs=probs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 07:29:28,419 WARNING From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "use a rnn layer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 07:29:28,727 WARNING From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "use 0.5 dropout layer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAaC4TwKvVrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.optimizers as opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaKx1llGv0Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUv6dojGv5ZR",
        "colab_type": "code",
        "outputId": "a65f856a-a4ef-43e1-b880-91667ef17eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sentence_input (InputLayer)  (None, None)              0         \n",
            "_________________________________________________________________\n",
            "word_emb (Embedding)         (None, None, 300)         3000900   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 3,723,003\n",
            "Trainable params: 3,723,003\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkb_hnUxv7Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(data1, data2, batch_size):\n",
        "    len_ = len(data1)\n",
        "    while True:\n",
        "        indices = np.random.choice(len_, batch_size)\n",
        "        x = data1[indices]\n",
        "        y = data2[indices]\n",
        "\n",
        "        maxlen = np.max([len(d) for d in x])\n",
        "        x = sequence.pad_sequences(x, maxlen)\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlHjVqtwwo3A",
        "colab_type": "text"
      },
      "source": [
        "### Training and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyk2qS8b0IBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g74qfcs7w3A4",
        "colab_type": "text"
      },
      "source": [
        "### For amazon_electronics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaolGaAMxTQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils.np_utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SW_SlNTw8L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rand_am = np.arange(len(data_list_am))\n",
        "np.random.shuffle(rand_am)\n",
        "\n",
        "data_list_am = data_list_am[rand_am]\n",
        "label_list_am = to_categorical(label_list_am)[rand_am]\n",
        "data_size_am = len(data_list_am)\n",
        "\n",
        "dev_x_am = data_list_am[0:1000]\n",
        "dev_y_am = label_list_am[0:1000]\n",
        "train_x_am = data_list_am[1000:int(data_size_am)]\n",
        "train_y_am = label_list_am[1000:int(data_size_am)]\n",
        "\n",
        "maxlen_am = np.max([len(d) for d in dev_x_am])\n",
        "dev_x_am = sequence.pad_sequences(dev_x_am, maxlen_am)\n",
        "\n",
        "import operator\n",
        "vocab_list_am = [x for (x, _) in sorted(vocab_am.items(), key=operator.itemgetter(1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IXtrag2xzIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen_am = batch_generator(train_x_am, train_y_am, batch_size=50)\n",
        "batches_per_ep_am = len(train_x_am) / 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUgsWeEgzdwR",
        "colab_type": "code",
        "outputId": "27cdd4f8-847a-4a24-b348-7ad147070405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "domain = 'amazon_electronics'\n",
        "best_acc = 0\n",
        "best_loss = 100\n",
        "for ii in range(10):\n",
        "    t0 = time()\n",
        "    loss, metric = 0., 0.\n",
        "\n",
        "    for b in tqdm(range(int(batches_per_ep_am))):\n",
        "        batch_x,  batch_y = train_gen_am.__next__()\n",
        "        loss_, metric_ = model.train_on_batch([batch_x], batch_y)\n",
        "        loss += loss_ / batches_per_ep_am\n",
        "        metric += metric_ / batches_per_ep_am\n",
        "\n",
        "    tr_time = time() - t0\n",
        "\n",
        "    dev_loss, dev_metric = model.evaluate([dev_x_am], dev_y_am, batch_size=50)\n",
        "\n",
        "    logger.info('Epoch %d, train: %is' % (ii, tr_time))\n",
        "    logger.info('[Train] loss: %.4f, metric: %.4f' % (loss, metric))\n",
        "    logger.info('[Dev] loss: %.4f, metric: %.4f' % (dev_loss, dev_metric))\n",
        "\n",
        "    if dev_metric > best_acc:\n",
        "      \n",
        "        best_acc = dev_metric\n",
        "        word_emb = model.get_layer('word_emb').get_weights()[0]\n",
        "        lstm_weights = model.get_layer('lstm').get_weights()\n",
        "        dense_weights = model.get_layer('dense').get_weights()\n",
        "\n",
        "        save_we(vocab_list_am, word_emb, domain)\n",
        "\n",
        "        if domain == 'amazon_electronics':\n",
        "            save_obj(lstm_weights, 'data/lstm_weights_lt')\n",
        "            save_obj(dense_weights, 'data/dense_weights_lt')\n",
        "        else:\n",
        "            save_obj(lstm_weights, 'data/lstm_weights_res')\n",
        "            save_obj(dense_weights, 'data/dense_weights_res')\n",
        "\n",
        "        print ('------- Saved Weights -------')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/580 [00:00<?, ?it/s]2019-04-06 21:08:09,527 WARNING From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "100%|██████████| 580/580 [15:09<00:00,  1.66s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 14s 14ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-06 21:23:33,334 INFO Epoch 0, train: 909s\n",
            "2019-04-06 21:23:33,340 INFO [Train] loss: 0.8933, metric: 0.5795\n",
            "2019-04-06 21:23:33,342 INFO [Dev] loss: 0.8037, metric: 0.6110\n",
            "  0%|          | 0/580 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------- Saved Weights -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 580/580 [14:59<00:00,  1.51s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 14s 14ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-06 21:38:48,160 INFO Epoch 1, train: 899s\n",
            "2019-04-06 21:38:48,162 INFO [Train] loss: 0.7343, metric: 0.6819\n",
            "2019-04-06 21:38:48,163 INFO [Dev] loss: 0.7622, metric: 0.6660\n",
            "  0%|          | 0/580 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------- Saved Weights -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 580/580 [15:00<00:00,  1.27s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 14s 14ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-06 21:54:02,915 INFO Epoch 2, train: 900s\n",
            "2019-04-06 21:54:02,916 INFO [Train] loss: 0.6735, metric: 0.7171\n",
            "2019-04-06 21:54:02,924 INFO [Dev] loss: 0.7839, metric: 0.6330\n",
            " 93%|█████████▎| 540/580 [14:06<01:08,  1.72s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DEGq_bOw5qK",
        "colab_type": "text"
      },
      "source": [
        "### For Yelp_2014"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yOdQkTewJ6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rand_yelp = np.arange(len(data_list_yelp))\n",
        "np.random.shuffle(rand_yelp)\n",
        "\n",
        "data_list_yelp = data_list_yelp[rand_yelp]\n",
        "label_list_yelp = to_categorical(label_list_yelp)[rand_yelp]\n",
        "data_size_yelp = len(data_list_yelp)\n",
        "\n",
        "dev_x_yelp = data_list_yelp[0:1000]\n",
        "dev_y_yelp = label_list_yelp[0:1000]\n",
        "train_x_yelp = data_list_yelp[1000:int(data_size_yelp)]\n",
        "train_y_yelp = label_list_yelp[1000:int(data_size_yelp)]\n",
        "\n",
        "maxlen_yelp = np.max([len(d) for d in dev_x_yelp])\n",
        "dev_x_yelp = sequence.pad_sequences(dev_x_yelp, maxlen_yelp)\n",
        "\n",
        "import operator\n",
        "vocab_list_yelp = [x for (x, _) in sorted(vocab_yelp.items(), key=operator.itemgetter(1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_apU4dZyFYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen_yelp = batch_generator(train_x_yelp, train_y_yelp, batch_size=50)\n",
        "batches_per_ep_yelp = len(train_x_yelp) / 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbgvUtqX7wu7",
        "colab_type": "code",
        "outputId": "4e7f4567-9b08-47e8-ad4c-7026a01d68c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        }
      },
      "source": [
        "domain = 'yelp14'\n",
        "best_acc = 0\n",
        "best_loss = 100\n",
        "for ii in range(10):\n",
        "    t0 = time()\n",
        "    loss, metric = 0., 0.\n",
        "\n",
        "    for b in tqdm(range(int(batches_per_ep_yelp))):\n",
        "        batch_x,  batch_y = train_gen_yelp.__next__()\n",
        "        loss_, metric_ = model.train_on_batch([batch_x], batch_y)\n",
        "        loss += loss_ / batches_per_ep_yelp\n",
        "        metric += metric_ / batches_per_ep_yelp\n",
        "\n",
        "    tr_time = time() - t0\n",
        "\n",
        "    dev_loss, dev_metric = model.evaluate([dev_x_yelp], dev_y_yelp, batch_size=50)\n",
        "\n",
        "    logger.info('Epoch %d, train: %is' % (ii, tr_time))\n",
        "    logger.info('[Train] loss: %.4f, metric: %.4f' % (loss, metric))\n",
        "    logger.info('[Dev] loss: %.4f, metric: %.4f' % (dev_loss, dev_metric))\n",
        "\n",
        "    if dev_metric > best_acc:\n",
        "      \n",
        "        best_acc = dev_metric\n",
        "        word_emb = model.get_layer('word_emb').get_weights()[0]\n",
        "        lstm_weights = model.get_layer('lstm').get_weights()\n",
        "        dense_weights = model.get_layer('dense').get_weights()\n",
        "\n",
        "        save_we(vocab_list_yelp, word_emb, domain)\n",
        "\n",
        "        if domain == 'amazon_electronics':\n",
        "            save_obj(lstm_weights, 'data/lstm_weights_lt')\n",
        "            save_obj(dense_weights, 'data/dense_weights_lt')\n",
        "        else:\n",
        "            save_obj(lstm_weights, 'data/lstm_weights_res')\n",
        "            save_obj(dense_weights, 'data/dense_weights_res')\n",
        "\n",
        "        print ('------- Saved Weights -------')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/580 [00:00<?, ?it/s]2019-04-07 07:30:11,768 WARNING From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "100%|██████████| 580/580 [16:29<00:00,  1.49s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 07:46:55,493 INFO Epoch 0, train: 989s\n",
            "2019-04-07 07:46:55,501 INFO [Train] loss: 0.7760, metric: 0.6606\n",
            "2019-04-07 07:46:55,503 INFO [Dev] loss: 0.6562, metric: 0.7190\n",
            "  0%|          | 0/580 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------- Saved Weights -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 580/580 [15:56<00:00,  1.87s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 08:03:06,841 INFO Epoch 1, train: 956s\n",
            "2019-04-07 08:03:06,845 INFO [Train] loss: 0.6060, metric: 0.7519\n",
            "2019-04-07 08:03:06,846 INFO [Dev] loss: 0.6630, metric: 0.7050\n",
            "100%|██████████| 580/580 [16:26<00:00,  1.31s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 08:19:47,893 INFO Epoch 2, train: 986s\n",
            "2019-04-07 08:19:47,895 INFO [Train] loss: 0.5439, metric: 0.7796\n",
            "2019-04-07 08:19:47,903 INFO [Dev] loss: 0.6130, metric: 0.7490\n",
            "  0%|          | 0/580 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------- Saved Weights -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 580/580 [16:15<00:00,  1.67s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 14s 14ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 08:36:17,500 INFO Epoch 3, train: 975s\n",
            "2019-04-07 08:36:17,501 INFO [Train] loss: 0.4892, metric: 0.8074\n",
            "2019-04-07 08:36:17,507 INFO [Dev] loss: 0.5719, metric: 0.7610\n",
            "  0%|          | 0/580 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------- Saved Weights -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 580/580 [15:51<00:00,  1.83s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 08:52:24,018 INFO Epoch 4, train: 951s\n",
            "2019-04-07 08:52:24,020 INFO [Train] loss: 0.4421, metric: 0.8263\n",
            "2019-04-07 08:52:24,026 INFO [Dev] loss: 0.5914, metric: 0.7570\n",
            "100%|██████████| 580/580 [16:05<00:00,  1.66s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 09:08:44,404 INFO Epoch 5, train: 965s\n",
            "2019-04-07 09:08:44,411 INFO [Train] loss: 0.4004, metric: 0.8468\n",
            "2019-04-07 09:08:44,413 INFO [Dev] loss: 0.5939, metric: 0.7610\n",
            "100%|██████████| 580/580 [16:00<00:00,  1.47s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 09:24:59,704 INFO Epoch 6, train: 960s\n",
            "2019-04-07 09:24:59,706 INFO [Train] loss: 0.3674, metric: 0.8598\n",
            "2019-04-07 09:24:59,707 INFO [Dev] loss: 0.6055, metric: 0.7690\n",
            "  0%|          | 0/580 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------- Saved Weights -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 580/580 [16:23<00:00,  1.78s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 09:41:38,383 INFO Epoch 7, train: 983s\n",
            "2019-04-07 09:41:38,385 INFO [Train] loss: 0.3264, metric: 0.8795\n",
            "2019-04-07 09:41:38,392 INFO [Dev] loss: 0.6629, metric: 0.7430\n",
            "100%|██████████| 580/580 [16:40<00:00,  1.66s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 09:58:34,159 INFO Epoch 8, train: 1000s\n",
            "2019-04-07 09:58:34,164 INFO [Train] loss: 0.2954, metric: 0.8916\n",
            "2019-04-07 09:58:34,166 INFO [Dev] loss: 0.6792, metric: 0.7600\n",
            "100%|██████████| 580/580 [16:29<00:00,  1.69s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-07 10:15:19,267 INFO Epoch 9, train: 989s\n",
            "2019-04-07 10:15:19,268 INFO [Train] loss: 0.2879, metric: 0.8964\n",
            "2019-04-07 10:15:19,273 INFO [Dev] loss: 0.6725, metric: 0.7650\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}