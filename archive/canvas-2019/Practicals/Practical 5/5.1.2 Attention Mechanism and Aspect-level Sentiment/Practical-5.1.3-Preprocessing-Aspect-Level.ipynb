{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0VnIJhDRpvc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_path = 'data/aspect_level-sentiment/aspect_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = 'data/doc_level-sentiment/doc_level'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "Pay attention how you represent your sequences as an input of RNN model.\n",
    "With a fixed length vector, you will need to pad the shorter sequences with \"0\".\n",
    "Consequently, your vocabulary indexing needs to consider this \"0\" as padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create vocabulary index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns:\n",
    "\n",
    "Python dictionary format of vocabulary indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMrMv9WhS23v"
   },
   "outputs": [],
   "source": [
    "num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
    "\n",
    "def is_number(token):\n",
    "    return bool(num_regex.match(token))\n",
    "\n",
    "\n",
    "def create_vocab(domain, aspect_path, doc_path, maxlen=0, vocab_size=0):\n",
    "    \n",
    "    assert domain in ['res_14', 'lt_14', 'res_15', 'res_16']\n",
    "\n",
    "    file_list = [os.path.join(aspect_path,'%s_train_sentence.txt'%(domain)),\n",
    "                 os.path.join(aspect_path,'%s_test_sentence.txt'%(domain))]\n",
    "\n",
    "    if domain in ['lt_14']:\n",
    "        file_list.append(os.path.join(doc_path,'amazon_electronics_text.txt'))\n",
    "    else:\n",
    "        file_list.append(os.path.join(doc_path,'yelp14_text.txt'))\n",
    "\n",
    "    print ('Creating vocab ...')\n",
    "\n",
    "    total_words, unique_words = 0, 0\n",
    "    word_freqs = {}\n",
    "\n",
    "    for f in file_list:\n",
    "        top = 0\n",
    "        fin = codecs.open(f, 'r', 'utf-8')\n",
    "        for line in fin:\n",
    "            words = line.split()\n",
    "            if maxlen > 0 and len(words) > maxlen:\n",
    "                continue\n",
    "            for w in words:\n",
    "                if not is_number(w):\n",
    "                    try:\n",
    "                        word_freqs[w] += 1\n",
    "                    except KeyError:\n",
    "                        unique_words += 1\n",
    "                        word_freqs[w] = 1\n",
    "                    total_words += 1\n",
    "\n",
    "    print ('  %i total words, %i unique words' % (total_words, unique_words))\n",
    "    sorted_word_freqs = sorted(word_freqs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    vocab = {'<pad>':0, '<unk>':1, '<num>':2}\n",
    "    index = len(vocab)\n",
    "    for word, _ in sorted_word_freqs:\n",
    "        vocab[word] = index\n",
    "        index += 1\n",
    "        if vocab_size > 0 and index > vocab_size + 2:\n",
    "            break\n",
    "    if vocab_size > 0:\n",
    "        print (' keep the top %i words' % vocab_size)\n",
    "\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_aspect(domain, aspect_path, phase, vocab, maxlen):\n",
    "    \n",
    "    assert domain in ['res_14', 'lt_14', 'res_15', 'res_16']\n",
    "    assert phase in ['train', 'test']\n",
    "    \n",
    "    print ('Preparing dataset ...')\n",
    "\n",
    "    data_x, data_y, aspect = [], [], []\n",
    "    polarity_category = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "    \n",
    "    if(phase == 'train'):\n",
    "        file_names = [os.path.join(aspect_path,'%s_%s_sentence.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path,'%s_%s_polarity.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path,'%s_%s_term.txt'%(domain, phase))]\n",
    "    else:\n",
    "        file_names = [os.path.join(aspect_path, '%s_%s_sentence.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path, '%s_%s_polarity.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path, '%s_%s_term.txt'%(domain, phase))]\n",
    "\n",
    "    num_hit, unk_hit, total = 0., 0., 0.\n",
    "    maxlen_x = 0\n",
    "    maxlen_aspect = 0\n",
    "\n",
    "    files = [open(i, 'r') for i in file_names]\n",
    "    for rows in zip(*files):\n",
    "        content = rows[0].strip().split()\n",
    "        polarity = rows[1].strip()\n",
    "        aspect_content = rows[2].strip().split()\n",
    "\n",
    "        if maxlen > 0 and len(content) > maxlen:\n",
    "            continue\n",
    "\n",
    "        content_indices = []\n",
    "        if len(content) == 0:\n",
    "            content_indices.append(vocab['<unk>'])\n",
    "            unk_hit += 1\n",
    "        for word in content:\n",
    "            if is_number(word):\n",
    "                content_indices.append(vocab['<num>'])\n",
    "                num_hit += 1\n",
    "            elif word in vocab:\n",
    "                content_indices.append(vocab[word])\n",
    "            else:\n",
    "                content_indices.append(vocab['<unk>'])\n",
    "                unk_hit += 1\n",
    "            total += 1\n",
    "\n",
    "        data_x.append(content_indices)\n",
    "        data_y.append(polarity_category[polarity])\n",
    "\n",
    "        aspect_indices = []\n",
    "        if len(aspect_content) == 0:\n",
    "            aspect_indices.append(vocab['<unk>'])\n",
    "            unk_hit += 1\n",
    "        for word in aspect_content:\n",
    "            if is_number(word):\n",
    "                aspect_indices.append(vocab['<num>'])\n",
    "            elif word in vocab:\n",
    "                aspect_indices.append(vocab[word])\n",
    "            else:\n",
    "                aspect_indices.append(vocab['<unk>'])\n",
    "        aspect.append(aspect_indices)\n",
    "\n",
    "        if maxlen_x < len(content_indices):\n",
    "            maxlen_x = len(content_indices)\n",
    "        if maxlen_aspect < len(aspect_indices):\n",
    "            maxlen_aspect = len(aspect_indices)\n",
    "\n",
    "\n",
    "    \n",
    "    print ('  <num> hit rate: %.2f%%, <unk> hit rate: %.2f%%' % (100*num_hit/total, 100*unk_hit/total))\n",
    "    return data_x, data_y, aspect, maxlen_x, maxlen_aspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_aspect(vocab, domain, aspect_path, maxlen=0):\n",
    "    \n",
    "    assert domain in ['res_14', 'lt_14', 'res_15', 'res_16']\n",
    "\n",
    "    train_x, train_y, train_aspect, train_maxlen, train_maxlen_aspect = \\\n",
    "    read_dataset_aspect(domain, aspect_path, 'train', vocab, maxlen)\n",
    "    \n",
    "    test_x, test_y, test_aspect, test_maxlen, test_maxlen_aspect = \\\n",
    "    read_dataset_aspect(domain, aspect_path, 'test', vocab, maxlen)\n",
    "    \n",
    "    overal_maxlen = max(train_maxlen, test_maxlen)\n",
    "    overal_maxlen_aspect = max(train_maxlen_aspect, test_maxlen_aspect)\n",
    "\n",
    "    print (' Overal_maxlen: %s' % overal_maxlen)\n",
    "    print (' Overal_maxlen_aspect:%s '% overal_maxlen_aspect)\n",
    "    \n",
    "    return train_x, train_y, train_aspect, test_x, test_y, test_aspect, overal_maxlen, overal_maxlen_aspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(vocab, text_path, label_path, skip_top, skip_len, replace_non_vocab):\n",
    "    \n",
    "    data = []\n",
    "    label = [] # {pos: 0, neg: 1, neu: 2}\n",
    "    f = codecs.open(text_path, 'r', 'utf-8')\n",
    "    f_l = codecs.open(label_path, 'r', 'utf-8')\n",
    "    num_hit, unk_hit, skip_top_hit, total = 0., 0., 0., 0.\n",
    "    pos_count, neg_count, neu_count = 0, 0, 0\n",
    "    max_len = 0\n",
    "\n",
    "    for line, score in zip(f, f_l):\n",
    "        word_indices = []\n",
    "        words = line.split()\n",
    "        if skip_len > 0 and len(words) > skip_len:\n",
    "            continue\n",
    "\n",
    "        score = float(score.strip())\n",
    "        if score < 3:\n",
    "            neg_count += 1\n",
    "            label.append(1)\n",
    "        elif score > 3:\n",
    "            pos_count += 1\n",
    "            label.append(0)\n",
    "        else:\n",
    "            neu_count += 1\n",
    "            label.append(2)\n",
    "            \n",
    "        for word in words:\n",
    "            if bool(num_regex.match(word)):\n",
    "                word_indices.append(vocab['<num>'])\n",
    "                num_hit += 1\n",
    "            elif word in vocab:\n",
    "                word_ind = vocab[word]\n",
    "                if skip_top > 0 and word_ind < skip_top + 3:\n",
    "                    skip_top_hit += 1\n",
    "                else:\n",
    "                    word_indices.append(word_ind)\n",
    "            else:\n",
    "                if replace_non_vocab:\n",
    "                    word_indices.append(vocab['<unk>'])\n",
    "                unk_hit += 1\n",
    "            total += 1\n",
    "\n",
    "        if len(word_indices) > max_len:\n",
    "            max_len = len(word_indices)\n",
    "\n",
    "        data.append(word_indices)\n",
    "\n",
    "    f.close()\n",
    "    f_l.close()\n",
    "\n",
    "    print('  <num> hit rate: %.2f%%, <unk> hit rate: %.2f%%' %(100*num_hit/total, 100*unk_hit/total))\n",
    "\n",
    "    return np.array(data), np.array(label), max_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_doc(vocab, domain, doc_path, skip_top=0, skip_len=0, replace_non_vocab=1):\n",
    "   \n",
    "    if domain in ['lt_14']:\n",
    "        text_path = os.path.join(doc_path,'amazon_electronics_text.txt')\n",
    "        score_path = os.path.join(doc_path,'amazon_electronics_label.txt')\n",
    "    else:\n",
    "        text_path= os.path.join(doc_path, 'yelp14_text.txt')\n",
    "        score_path = os.path.join(doc_path,'yelp14_label.txt')\n",
    "\n",
    "    data, label, max_len = create_data(vocab, text_path, score_path, skip_top, skip_len, replace_non_vocab)\n",
    "\n",
    "    return data, label, max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(domain, aspect_path, doc_path, vocab_size, maxlen=0):\n",
    "    \n",
    "    vocab = create_vocab(domain, aspect_path, doc_path, maxlen, vocab_size)\n",
    "\n",
    "    train_x, train_y, train_aspect, test_x, test_y, \\\n",
    "    test_aspect, overal_maxlen, overal_maxlen_aspect = get_data_aspect(vocab, domain, aspect_path)\n",
    "\n",
    "    pretrain_data, pretrain_label, pretrain_maxlen = prepare_data_doc(vocab, domain, doc_path)\n",
    "\n",
    "    return train_x, train_y, train_aspect, test_x, test_y, \\\n",
    "test_aspect, vocab, overal_maxlen, overal_maxlen_aspect, pretrain_data, pretrain_label, pretrain_maxlen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence preprocessing (for model inputs - outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eO0eUGw6VG5r",
    "outputId": "ee8dd7b3-0c65-46b9-a8a5-c0607668a7f0"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "WOc3Q2sNVcl6",
    "outputId": "39fa40be-706b-4133-8898-90c862e23f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab ...\n",
      "  3498349 total words, 39278 unique words\n",
      " keep the top 10000 words\n",
      "Preparing dataset ...\n",
      "  <num> hit rate: 0.99%, <unk> hit rate: 1.19%\n",
      "Preparing dataset ...\n",
      "  <num> hit rate: 1.18%, <unk> hit rate: 1.13%\n",
      " Overal_maxlen: 82\n",
      " Overal_maxlen_aspect:7 \n",
      "  <num> hit rate: 1.04%, <unk> hit rate: 1.56%\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, train_aspect, test_x, test_y, test_aspect, \\\n",
    "    vocab, overal_maxlen, overal_maxlen_aspect, \\\n",
    "    pretrain_data, pretrain_label, pretrain_maxlen = prepare_data('lt_14', aspect_path, doc_path, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gl50euzvDcQP",
    "outputId": "54082e09-e6db-4178-e8b2-b8b86a0f51a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation size: 462\n"
     ]
    }
   ],
   "source": [
    "# Pad aspect sentences sequences for mini-batch processing\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)\n",
    "train_aspect = sequence.pad_sequences(train_aspect, maxlen=overal_maxlen_aspect)\n",
    "test_aspect = sequence.pad_sequences(test_aspect, maxlen=overal_maxlen_aspect)\n",
    "\n",
    "#maxlen_pretrain = np.max([len(d) for d in pretrain_data])\n",
    "maxlen_pretrain = 300\n",
    "pretrain_data = sequence.pad_sequences(pretrain_data, maxlen_pretrain)\n",
    "\n",
    "# convert y to categorical labels\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "pretrain_label = to_categorical(pretrain_label, 3)\n",
    "\n",
    "validation_ratio = 0.2\n",
    "validation_size = int(len(train_x) * validation_ratio)\n",
    "print ('Validation size: %s' % validation_size)\n",
    "\n",
    "\n",
    "dev_x = train_x[:validation_size]\n",
    "dev_y = train_y[:validation_size]\n",
    "dev_aspect = train_aspect[:validation_size]\n",
    "\n",
    "train_x = train_x[validation_size:]\n",
    "train_y = train_y[validation_size:]\n",
    "train_aspect = train_aspect[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('infrequent', 6585), ('printer', 686), ('consistent', 2620), ('hot', 909), ('ballhead', 9017)]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overal_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overal_maxlen_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_data.shape # data from doc-level domain (input sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_label.shape # data from doc-level domain (output labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_maxlen # max sequence length of input from doc-level domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1851, 82)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape # data from aspect-level domain (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1851, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape # data from aspect-level domain (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 82)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(638, 82)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape # data from aspect-level domain (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(638, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape # data from aspect-level domain (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1851, 7)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_aspect.shape # aspect words (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(638, 7)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_aspect.shape # aspect words (test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file saved to: data/aspect_level-sentiment/aspect_level/all_vocab.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/train_x.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/train_y.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/dev_x.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/dev_y.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/test_x.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/test_y.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/train_aspect.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/dev_aspect.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/test_aspect.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/pretrain_data.pkl\n",
      " file saved to: data/aspect_level-sentiment/aspect_level/pretrain_label.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pickle(aspect_path, 'all_vocab.pkl', vocab)\n",
    "\n",
    "save_pickle(aspect_path, 'train_x.pkl', train_x)\n",
    "save_pickle(aspect_path, 'train_y.pkl', train_y)\n",
    "save_pickle(aspect_path, 'dev_x.pkl', dev_x)\n",
    "save_pickle(aspect_path, 'dev_y.pkl', dev_y)\n",
    "save_pickle(aspect_path, 'test_x.pkl', test_x)\n",
    "save_pickle(aspect_path, 'test_y.pkl', test_y)\n",
    "\n",
    "save_pickle(aspect_path, 'train_aspect.pkl', train_aspect)\n",
    "save_pickle(aspect_path, 'dev_aspect.pkl', dev_aspect)\n",
    "save_pickle(aspect_path, 'test_aspect.pkl', test_aspect)\n",
    "\n",
    "\n",
    "save_pickle(aspect_path, 'pretrain_data.pkl', pretrain_data)\n",
    "save_pickle(aspect_path, 'pretrain_label.pkl', pretrain_label)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Aspect_Level_res_14.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
