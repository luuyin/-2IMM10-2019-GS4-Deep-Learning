{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We can load a pre-trained CNN directly through Keras. We choose VGG16, which consists of convolutional and pooling layers first, followed by a few fully connected (dense) layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a model that omits the last layer of the VGG16 net, so we can use it to obtain feature vectors for any given input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc2_model = Model(inputs=base_model.input, outputs=base_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image examples\n",
    "Let's look at a few example images. We load the image, transform it to a numpy array, and preprocess it for use with VGG16. We store the intermediate steps in dictionaries, for demonstrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_preprocess(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    array = image.img_to_array(img)\n",
    "    x = np.expand_dims(array, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return {\"img\": img, \"array\": array, \"x\": x}\n",
    "\n",
    "elephant1 = load_img_preprocess('images/elephant1.jpg')\n",
    "elephant2 = load_img_preprocess('images/elephant2.jpg')\n",
    "hippo1 = load_img_preprocess('images/hippo1.jpg')\n",
    "\n",
    "def show_image_predictions(img_obj):\n",
    "    plt.imshow(img_obj[\"img\"])\n",
    "    plt.show()\n",
    "    preds = base_model.predict(img_obj[\"x\"])\n",
    "    preds_dec = decode_predictions(preds, top=5)[0]\n",
    "    print(\"Predictions:\")\n",
    "    for pred in preds_dec:\n",
    "        print(\"{}, with probability: {}\".format(pred[1],pred[2]))\n",
    "    print(\"\")\n",
    "\n",
    "show_image_predictions(elephant1)\n",
    "show_image_predictions(elephant2)\n",
    "show_image_predictions(hippo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the fc2_model to obtain feature vectors for our example images, and include them in the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def include_features(img_obj):\n",
    "    img_obj[\"fc2\"] = fc2_model.predict(img_obj[\"x\"])\n",
    "    \n",
    "include_features(elephant1)\n",
    "include_features(elephant2)\n",
    "include_features(hippo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephant1[\"x\"].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare two images (as matrices) or feature vectors by computing the average (squared) distance elementwise. Let's do this for our example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean L1-distance (mean absolute error)\n",
    "def mae(a,b):\n",
    "    return np.absolute(a-b).mean(axis=None)\n",
    "    \n",
    "# mean squared L2-distance (mean squared error)\n",
    "def mse(a,b):\n",
    "    return ((a-b)**2).mean(axis=None)\n",
    "\n",
    "print(\"fc2 feature vectors:\")\n",
    "print(mse(elephant1[\"fc2\"], elephant2[\"fc2\"]))\n",
    "print(mse(elephant1[\"fc2\"], hippo1[\"fc2\"]))\n",
    "print(mse(elephant2[\"fc2\"], hippo1[\"fc2\"]))\n",
    "print(\"\")\n",
    "print(mae(elephant1[\"fc2\"], elephant2[\"fc2\"]))\n",
    "print(mae(elephant1[\"fc2\"], hippo1[\"fc2\"]))\n",
    "print(mae(elephant2[\"fc2\"], hippo1[\"fc2\"]))\n",
    "print(\"\")\n",
    "print(\"original images as 224x224 matrices:\")\n",
    "print(mse(elephant1[\"array\"].flatten(), elephant2[\"array\"].flatten()))\n",
    "print(mse(elephant1[\"array\"].flatten(), hippo1[\"array\"].flatten()))\n",
    "print(mse(elephant2[\"array\"].flatten(), hippo1[\"array\"].flatten()))\n",
    "print(\"\")\n",
    "print(mae(elephant1[\"array\"].flatten(), elephant2[\"array\"].flatten()))\n",
    "print(mae(elephant1[\"array\"].flatten(), hippo1[\"array\"].flatten()))\n",
    "print(mae(elephant2[\"array\"].flatten(), hippo1[\"array\"].flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Observe that when using the feature vectors, the first two images (both elephants) are considered \"more similar\" according to these distance measures. Looking at the original images as 224x224 matrices however, the first elephant is more similar to the hippo, according to these distance measures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
