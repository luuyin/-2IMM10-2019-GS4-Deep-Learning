{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Sequential Model for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use IMDB review data set for generating the encoding of sentence (i.e text review from user) to classify sentiment polarity of this text. Data is originally taken from https://www.kaggle.com/c/word2vec-nlp-tutorial/data. It contains 25000 reviews with labels 0 for \"negative\" sentiment and 1 for \"positive\" sentiment. For validation set, the information about binary labels (0 and 1) can be seen in attribute \"id\" of the data set. Number after character '\\_' represents rating score. If rating <5, then the sentiment score is 0 or \"negative\" sentiment. If the rating >=7, then the score is 1 or \"positive. Otherwise, is negative. For test set, data is provided without label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of (part of) original text in data set:\n",
    "\n",
    "```\n",
    "id\tsentiment\treview\n",
    "\n",
    "\"7759_3\"\t0\t\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger .\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a text (e.g. a movie review), we will predict whether this review is positive (class label=1) or negative (class label =0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tasks:\n",
    "\n",
    "* Encode text using LSTM as an encoder layer.\n",
    "* Project the output of encoder model to dense prediction layer.\n",
    "* Train the model with objective function to minimize error loss of sentiment classification task, given the encoding of text sequence as the input of the model.\n",
    "\n",
    "YOUR ASSIGNMENT TASKS:\n",
    "* <span style=\"color:red\">Plot error loss and accuracy in training and validation stage. Discuss the result.</span>\n",
    "* <span style=\"color:red\">Generate document embedding from RNN layer that has been trained and optimized to the sentiment classification task.</span>\n",
    "* <span style=\"color:red\">Visualize the resulting document embedding and project to their sentiment labels in embedding space.</span>\n",
    "* <span style=\"color:red\">Test with new set of document (raw data set is provided).</span>\n",
    "* <span style=\"color:red\">Assign new labels to new unseen and unlabelled document. You will need to encode this raw document as well as a query to source (trained) document embedding. Sample only 10 new unlabelled documents.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Read preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "DATA_PATH = 'data/imdb'\n",
    "MODEL_PATH = 'model/assignment_3_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "\n",
    "# reading file in pickle format\n",
    "def readPickle(pickleFilename):\n",
    "    f = open(pickleFilename, 'rb')\n",
    "    obj = cPickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePickle(dataToWrite,pickleFilename):\n",
    "    f = open(pickleFilename, 'wb')\n",
    "    cPickle.dump(dataToWrite, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(html):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_PATH,\"labeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv(os.path.join(DATA_PATH,\"testData.tsv\"), header=0, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''\n",
    "docs = []\n",
    "sentiments = []\n",
    "for cont, sentiment in zip(data.review, data.sentiment):\n",
    "    doc = clean(striphtml(cont))\n",
    "    doc = doc.lower() \n",
    "    docs.append(doc)\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_docs =[]\n",
    "valid_labels = []\n",
    "i=0\n",
    "for docid,cont in zip(valid_data.id, valid_data.review):\n",
    "    id_label = docid.split('_')\n",
    "    if(int(id_label[1]) >= 7):\n",
    "        valid_labels.append(1)\n",
    "    else:\n",
    "        valid_labels.append(0)         \n",
    "    doc = clean(striphtml(cont))\n",
    "    doc = doc.lower() \n",
    "    valid_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeWords(text):\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).split()\n",
    "    return [str(strtokens) for strtokens in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexingVocabulary(array_of_words):\n",
    "\n",
    "    wordIndex = list(array_of_words)\n",
    "    wordIndex.insert(0,'</PAD>')\n",
    "    if 'sof' not in array_of_words:\n",
    "        wordIndex.append('</START_DOC>')\n",
    "    if 'eof' not in array_of_words:\n",
    "        wordIndex.append('</END_DOC>')\n",
    "    wordIndex.append('</UNK>')\n",
    "    vocab=dict([(i,wordIndex[i]) for i in range(len(wordIndex))])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str_tokens = []\n",
    "all_tokens = []\n",
    "for i, text in enumerate(docs):\n",
    "    # tokenize text \n",
    "    train_str_tokens.append(tokenizeWords(text))\n",
    "    all_tokens.extend(tokenizeWords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with']\n"
     ]
    }
   ],
   "source": [
    "print(train_str_tokens[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_str_tokens = []\n",
    "for i, text in enumerate(valid_docs):\n",
    "    # tokenize text \n",
    "    valid_str_tokens.append(tokenizeWords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = nltk.FreqDist(all_tokens)\n",
    "common_words = tf.most_common(5000)\n",
    "arr_common = np.array(common_words)\n",
    "words = arr_common[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_indices = indexingVocabulary(words)\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '</PAD>'), (1, 'the'), (2, 'and'), (3, 'a'), (4, 'of')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(words_indices.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('treated', 1908),\n",
       " ('has', 46),\n",
       " ('olivier', 4092),\n",
       " ('remake', 1014),\n",
       " ('starring', 1180)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(indices_words.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer format of training input \n",
    "train_int_input = []\n",
    "for i, text in enumerate(train_str_tokens):\n",
    "    int_tokens = [indices_words[w] if w in indices_words.keys() else indices_words['</UNK>'] for w in text ]\n",
    "    train_int_input.append(int_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer format of test validation input \n",
    "valid_int_input = []\n",
    "for i, text in enumerate(valid_str_tokens):\n",
    "    int_tokens = [indices_words[w] if w in indices_words.keys() else indices_words['</UNK>'] for w in text ]\n",
    "    valid_int_input.append(int_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_int_input)\n",
    "y_train = np.array(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.array(valid_int_input)\n",
    "y_valid = np.array(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing training and validation set\n",
    "savePickle(X_train, os.path.join(DATA_PATH,'X_train'))\n",
    "savePickle(y_train, os.path.join(DATA_PATH,'y_train'))\n",
    "savePickle(X_valid, os.path.join(DATA_PATH,'X_valid'))\n",
    "savePickle(y_valid, os.path.join(DATA_PATH,'y_valid'))\n",
    "# storing look-up dictionary for vocabulary index\n",
    "savePickle(words_indices, os.path.join(DATA_PATH,'words_indices'))\n",
    "savePickle(indices_words, os.path.join(DATA_PATH,'indices_words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE TO PREPARE THE ENCODING OF NEW UNSEEN UNLABELED TEST DATA\n",
    "\n",
    "# 1. Read file data/unlabeledTrainData.csv (ONLY USE FIRST 25000 DOCUMENTS)\n",
    "# 2. Do similar preprocessing as in training and validation set\n",
    "# 3. Encode to integer format of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Word-level document encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train_pad = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_valid_pad = sequence.pad_sequences(X_valid, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_encoder (Embedding (None, None, 32)          160128    \n",
      "_________________________________________________________________\n",
      "lstm_encoder (LSTM)          (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,429\n",
      "Trainable params: 213,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "embedding_dim = 32\n",
    "\n",
    "encoder_input = Input(shape=(None,), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(len(words_indices), embedding_dim, name='embedding_encoder')(encoder_input)\n",
    "lstm_encoder = LSTM(latent_dim, name='lstm_encoder')(encoder_embedding)\n",
    "output_encoder = Dense(1, activation='sigmoid')(lstm_encoder)\n",
    "model = Model(inputs=encoder_input, outputs=output_encoder)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 695s 28ms/step - loss: 0.4818 - acc: 0.7784 - val_loss: 0.3527 - val_acc: 0.8624\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 664s 27ms/step - loss: 0.3365 - acc: 0.8692 - val_loss: 0.3146 - val_acc: 0.8710\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 711s 28ms/step - loss: 0.2690 - acc: 0.8942 - val_loss: 0.3086 - val_acc: 0.8758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2fe188a20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, validation_data=(X_valid_pad, y_valid), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# CHANGE BELOW CODE FOR TRAINING THE MODEL\n",
    "# 1. Increase epoch number for inspecting the error loss through epochs \n",
    "#    (optional - if your computation resource is sufficient)\n",
    "# 2. Add callback function for historical error loss and accuracy during training and validation stage\n",
    "# 3. Plot history of error loss and accuracy (with matplotlib or any available library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save the trained models and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(os.path.join(MODEL_PATH,'word_sequence_classification_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained weight parameters\n",
    "model.save_weights(os.path.join(MODEL_PATH, 'weights_word_sequence_classification.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Retrieve the encoding of document that has been optimized to sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input: validation set\n",
    "* output: document embedding of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Generate document embedding from trained model and parameters. \n",
    "#    There are several ways to retrieve document embedding from LSTM encoder layer. Choose one.\n",
    "# 2. Visualize w.r.t. sentiment labels (Use tSNE for dimensionality reduction)\n",
    "# 3. Evaluate the quality of document embedding on subsequent binary classification task, by:\n",
    "\n",
    "#    - loss and accuracy of trained model on new unseen documents\n",
    "\n",
    "#    - MLP classifier built in keras model. Justify your chosen architecture.\n",
    "\n",
    "#    - Linear model / SVM classifier \n",
    "\n",
    "#    IMPORTANT: your task is not optimizing classifier or finetuning it -  but evaluating the quality of embedding\n",
    "#    with the most minimal parameter settings of classifier\n",
    "#    Discuss the results and if anys - a better way to evaluate the quality of embeddings.\n",
    "\n",
    "# You may need to store your encoder model, full model, and weights for the next task (TASK 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Document similarity task from new unseen documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Generate document embedding from the trained model and new unseen document (preprocessed unlabelled data)\n",
    "# 2. Sample 10 unseen unlabelled document embedding and compute document similarity with the previous \n",
    "#    resulting labelled document embedding\n",
    "#    - compute document similarity\n",
    "#    - classify new 10 document embeddings according to the similarity measurement \n",
    "#       define the decision making to assign these new 10 labels\n",
    "# 3. Visualize the result of additional 10 new unseen documents and evaluate similarity results. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
