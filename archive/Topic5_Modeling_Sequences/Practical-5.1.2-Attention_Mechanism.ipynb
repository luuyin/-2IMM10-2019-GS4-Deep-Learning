{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/doc_level-sentiment/doc_level'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_idx = read_pickle(data_path, 'words_idx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_words = read_pickle(data_path, 'idx_words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_pickle(data_path, 'data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = read_pickle(data_path, 'label.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.arange(len(data))\n",
    "np.random.shuffle(rand_idx)\n",
    "\n",
    "data = data[rand_idx]\n",
    "label = to_categorical(label)[rand_idx]\n",
    "\n",
    "data_size = len(data)\n",
    "\n",
    "test_x = data[0:1000]\n",
    "test_y = label[0:1000]\n",
    "\n",
    "dev_x = data[1000:5000]\n",
    "dev_y = label[1000:5000]\n",
    "\n",
    "train_x = data[5000:int(data_size)]\n",
    "train_y = label[5000:int(data_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = np.max([len(d) for d in dev_x])\n",
    "\n",
    "import operator\n",
    "words_idx = [x for (x, _) in sorted(words_idx.items(), key=operator.itemgetter(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ = sequence.pad_sequences(train_x, maxlen)\n",
    "dev_x_ = sequence.pad_sequences(dev_x, maxlen)\n",
    "test_x_ = sequence.pad_sequences(test_x, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ = np.array(train_x_)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "dev_x_ = np.array(dev_x_)\n",
    "dev_y = np.array(dev_y)\n",
    "\n",
    "test_x_ = np.array(test_x_)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataiterator():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, seq_length=32, decoder_dim=300, batch_size=32):      \n",
    "        self.X = X \n",
    "        self.y = y \n",
    "        self.p0 = np.zeros((len(X), 3))\n",
    "        self.num_data = len(X) # total number of examples\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        \n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X = self.X[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y = self.y[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        batch_p0 = self.p0[np.array(X_ids)]\n",
    "        \n",
    "        return batch_X, batch_y, batch_p0\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X, self.y, self.p0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13GNLSaWu7np"
   },
   "source": [
    "### LSTM Model for document level sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0I4Ob1uvL3O"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Input, RepeatVector, Dot, Concatenate, Reshape\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words_idx) \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_softmax(x, axis=1):\n",
    "            \"\"\"Softmax activation function.\n",
    "            # Arguments\n",
    "                x : Tensor.\n",
    "                axis: Integer, axis along which the softmax normalization is applied.\n",
    "            # Returns\n",
    "                Tensor, output of softmax transformation.\n",
    "            # Raises\n",
    "                ValueError: In case `dim(x) == 1`.\n",
    "            \"\"\"\n",
    "            ndim = K.ndim(x)\n",
    "            if ndim == 2:\n",
    "                return K.softmax(x)\n",
    "            elif ndim > 2:\n",
    "                e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "                s = K.sum(e, axis=axis, keepdims=True)\n",
    "                return e / s\n",
    "            else:\n",
    "                raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeator = RepeatVector(maxlen, name='repeator_att')\n",
    "concatenator = Concatenate(axis=-1, name='concator_att')\n",
    "densor1 = Dense(300, activation = \"tanh\", name='densor1_att')\n",
    "densor2 = Dense(1, activation = \"relu\", name='densor2_att')\n",
    "activator = Activation(custom_softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1, name='dotor_att')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the attention inputs (keys , query) in this sentiment classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(keys, query):\n",
    "    \n",
    "    query = repeator(query)\n",
    "    print(\"query shape: %s\" %str(query._keras_shape))\n",
    "    concat = concatenator([keys, query])\n",
    "    print(\"concat shape: %s\" %str(concat._keras_shape))\n",
    "    e1 = densor1(concat)\n",
    "    print(\"e1 shape: %s\" %str(e1._keras_shape))\n",
    "    e2 = densor2(e1)\n",
    "    print(\"e2 shape: %s\" %str(e2._keras_shape))\n",
    "    alphas = activator(e2)\n",
    "    print(\"alphas shape: %s\" %str(alphas._keras_shape))\n",
    "    context = dotor([alphas, keys])\n",
    "    print(\"context shape: %s\" %str(context._keras_shape))\n",
    "    \n",
    "    return context, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentence / document input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Query input for attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND REPLACE '?' WITH A CORRECT NUMBER OF DIMENSION\n",
    "\n",
    "#p0 = Input(shape=(?,), name='p0')\n",
    "#probs = [p0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer to train embedding weights of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-based layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get attention weights and attention-based context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNCOMMENT AND REPLACE KEYS?, QUERY? WITH THE CORRESPONDING TENSORS AS ATTENTION KEYS AND QUERY\n",
    "\n",
    "#print ('use content attention to get term weights')\n",
    "#att_context, att_weights = attention(KEYS?, QUERY?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND REPLACE '?' WITH A CORRECT NUMBER OF DIMENSION\n",
    "\n",
    "#att_context = Reshape((?,))(att_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [probs] # Output from last softmax activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "model = Model(inputs=[sentence_input, p0], outputs=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAaC4TwKvVrF"
   },
   "outputs": [],
   "source": [
    "import keras.optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaKx1llGv0Zp"
   },
   "outputs": [],
   "source": [
    "optimizer = opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "yUv6dojGv5ZR",
    "outputId": "a65f856a-a4ef-43e1-b880-91667ef17eac"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_apU4dZyFYE"
   },
   "outputs": [],
   "source": [
    "train_steps_epoch = len(train_x_)/batch_size\n",
    "batch_train_iter = Dataiterator(train_x_, train_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps_epoch = len(dev_x_)/batch_size\n",
    "batch_val_iter = Dataiterator(dev_x_, dev_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_generator(model, batch_train_iter, batch_val_iter):\n",
    "    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
    "                                     monitor='val_loss', save_best_only=False, \\\n",
    "                                     save_weights_only=True)\n",
    "                     ]\n",
    "    \n",
    "    def train_gen():\n",
    "        while True:\n",
    "            train_batches = [[[X, p0], y] for X, y, p0 in batch_train_iter]\n",
    "            for train_batch in train_batches:\n",
    "                yield train_batch\n",
    "                \n",
    "    def val_gen():\n",
    "        while True:\n",
    "            val_batches = [[[X, p0], y] for X, y, p0 in batch_val_iter]\n",
    "            for val_batch in val_batches:\n",
    "                yield val_batch\n",
    "                \n",
    "    history = model.fit_generator(train_gen(), validation_data=val_gen(), \\\n",
    "                                  validation_steps=val_steps_epoch, steps_per_epoch=train_steps_epoch, \\\n",
    "                                  epochs = 20, callbacks = earlystop_callbacks)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator(model, batch_train_iter, batch_val_iter)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Doc_level_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
