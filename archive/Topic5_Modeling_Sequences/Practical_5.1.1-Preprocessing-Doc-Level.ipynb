{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTvH9Q5vygvV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/doc_level-sentiment/doc_level'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "Pay attention how you represent your sequences as an input of RNN model.\n",
    "With a fixed length vector, you will need to pad the shorter sequences with \"0\".\n",
    "Consequently, your vocabulary indexing needs to consider this \"0\" as padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create vocabulary index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns:\n",
    "\n",
    "Python dictionary format of vocabulary indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhiEyxIRm_HK"
   },
   "outputs": [],
   "source": [
    "num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
    "\n",
    "def create_vocab(domain, data_path, maxlen=0, vocab_size=0):\n",
    "    \n",
    "    print('Creating vocab ...')\n",
    "\n",
    "    f = os.path.join(data_path,'%s_text.txt'%(domain))\n",
    "\n",
    "    total_words, unique_words = 0, 0\n",
    "    word_freqs = {}\n",
    "\n",
    "    fin = codecs.open(f, 'r', 'utf-8')\n",
    "    for line in fin:\n",
    "        words = line.split()\n",
    "        if maxlen > 0 and len(words) > maxlen:\n",
    "            continue\n",
    "\n",
    "        for w in words:\n",
    "            if not bool(num_regex.match(w)):\n",
    "                try:\n",
    "                    word_freqs[w] += 1\n",
    "                except KeyError:\n",
    "                    unique_words += 1\n",
    "                    word_freqs[w] = 1\n",
    "                total_words += 1\n",
    "\n",
    "    print ('  %i total words, %i unique words' % (total_words, unique_words))\n",
    "    sorted_word_freqs = sorted(word_freqs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    vocab = {'<pad>':0, '<unk>':1, '<num>':2}\n",
    "    index = len(vocab)\n",
    "    for word, _ in sorted_word_freqs:\n",
    "        vocab[word] = index\n",
    "        index += 1\n",
    "        if vocab_size > 0 and index > vocab_size + 2:\n",
    "            break\n",
    "    if vocab_size > 0:\n",
    "        print (' keep the top %i words' % vocab_size)\n",
    "\n",
    "  \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to transform word sequence -> integer sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "The raw data set has 5 class labels. Here, we only consider 3 sentiment classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns:\n",
    "\n",
    "integer sequence of text, corresponding labels (int), maxlen (maximum length of sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(vocab, text_path, label_path, domain, skip_top, skip_len, replace_non_vocab):\n",
    "    \n",
    "    data = []\n",
    "    label = [] # {pos: 0, neg: 1, neu: 2}\n",
    "    \n",
    "    f = codecs.open(text_path, 'r', 'utf-8')\n",
    "    f_l = codecs.open(label_path, 'r', 'utf-8')\n",
    "    \n",
    "    num_hit, unk_hit, skip_top_hit, total = 0., 0., 0., 0.\n",
    "    pos_count, neg_count, neu_count = 0, 0, 0\n",
    "    max_len = 0\n",
    "\n",
    "    for line, score in zip(f, f_l):\n",
    "        word_indices = []\n",
    "        words = line.split()\n",
    "        if skip_len > 0 and len(words) > skip_len:\n",
    "            continue\n",
    "\n",
    "        score = float(score.strip())\n",
    "        if score < 3:\n",
    "            neg_count += 1\n",
    "            label.append(1)\n",
    "        elif score > 3:\n",
    "            pos_count += 1\n",
    "            label.append(0)\n",
    "        else:\n",
    "            neu_count += 1\n",
    "            label.append(2)\n",
    "          \n",
    "        for word in words:\n",
    "            if bool(num_regex.match(word)):\n",
    "                word_indices.append(vocab['<num>'])\n",
    "                num_hit += 1\n",
    "            elif word in vocab:\n",
    "                word_ind = vocab[word]\n",
    "                if skip_top > 0 and word_ind < skip_top + 3:\n",
    "                    skip_top_hit += 1\n",
    "                else:\n",
    "                    word_indices.append(word_ind)\n",
    "            else:\n",
    "                if replace_non_vocab:\n",
    "                    word_indices.append(vocab['<unk>'])\n",
    "                unk_hit += 1\n",
    "            total += 1\n",
    "\n",
    "        if len(word_indices) > max_len:\n",
    "            max_len = len(word_indices)\n",
    "\n",
    "        data.append(word_indices)\n",
    "\n",
    "    f.close()\n",
    "    f_l.close()\n",
    "\n",
    "    print('  <num> hit rate: %.2f%%, <unk> hit rate: %.2f%%' % (100*num_hit/total, 100*unk_hit/total))\n",
    "\n",
    "    print (domain)\n",
    "    print( 'pos count: ', pos_count )\n",
    "    print( 'neg count: ', neg_count )\n",
    "    print( 'neu count: ', neu_count )\n",
    "\n",
    "    return np.array(data), np.array(label), max_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call : \n",
    "\n",
    "- create_vocab()\n",
    "- create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return\n",
    "\n",
    "- vocabulary index\n",
    "- integer sequence (model input)\n",
    "- label (model output)\n",
    "- maximum sequence length -> as parameter for RNN / LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(domain, data_path, vocab_size, skip_top=0, skip_len=0, replace_non_vocab=1):\n",
    "    \n",
    "    print(domain)\n",
    "\n",
    "    assert domain in ['amazon_electronics', 'yelp14']\n",
    "\n",
    "    vocab = create_vocab(domain, data_path, skip_len, vocab_size)\n",
    "\n",
    "    text_path = os.path.join(data_path,'%s_text.txt'%(domain))\n",
    "    score_path = os.path.join(data_path,'%s_label.txt'%(domain))\n",
    "\n",
    "    data, label, max_len = create_data(vocab, text_path, score_path, domain, skip_top, \\\n",
    "                                       skip_len, replace_non_vocab)\n",
    "\n",
    "    return vocab, data, label, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose domain data to train\n",
    "domain_name = 'amazon_electronics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Cp_MkulGr5RZ",
    "outputId": "6f69104f-abd2-4517-da1a-b3e2d02ec0a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_electronics\n",
      "Creating vocab ...\n",
      "  3440972 total words, 39122 unique words\n",
      " keep the top 10000 words\n",
      "  <num> hit rate: 1.04%, <unk> hit rate: 1.56%\n",
      "amazon_electronics\n",
      "pos count:  10000\n",
      "neg count:  10000\n",
      "neu count:  10000\n"
     ]
    }
   ],
   "source": [
    "vocab, data_list, label_list, overall_maxlen = prepare_data(domain_name, data_path, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168, 109, 12, 791, 11, 17, 2207, 218, 9, 48, 1, 117, 3, 1318, 9181, 37, 1003, 16, 3, 535, 74, 8, 806, 6, 134, 348, 1610, 60, 523, 522, 6, 540, 10, 35, 149, 8, 458, 3412, 51, 104, 10, 39, 113, 6, 4918, 15, 13, 105, 173, 61, 42, 8, 173, 36, 47, 64, 61, 11, 3, 657, 825, 25, 69, 20, 93, 21, 3, 9181, 79, 760, 3, 236, 2175, 5047, 2258, 14, 42, 6, 3243, 9, 905, 51, 8165, 301, 3, 951, 211, 3, 1432, 10, 35, 3, 1432, 10, 994, 1944, 114, 343, 308, 89, 185, 5, 174, 56, 3, 105, 16, 3, 2520, 10, 8, 3412, 66, 9, 259, 31, 15, 6, 4932, 7, 13, 24, 39, 17, 856, 4, 1469, 3, 1099, 7, 522, 368, 10, 17, 1040, 4, 437, 6, 1040, 101, 16, 630, 11, 48, 1873, 36, 37, 1003, 41, 3, 121, 138, 36]\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(label_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to access the stored vocabulary indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('requiring', 2350), ('reach', 1311), ('defective', 879), ('seattle', 8930), ('debate', 8876)]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkAMTb8lt0Xs"
   },
   "outputs": [],
   "source": [
    "idx_words = dict((v,k) for (k,v) in vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '<pad>'), (1, '<unk>'), (2, '<num>'), (3, 'the'), (4, 'i')]\n"
     ]
    }
   ],
   "source": [
    "print(list(idx_words.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing all preprocessing data\n",
    "\n",
    "Here, we store as a pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file saved to: data/doc_level-sentiment/doc_level/words_idx.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pickle(data_path, 'words_idx.pkl', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file saved to: data/doc_level-sentiment/doc_level/idx_words.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pickle(data_path, 'idx_words.pkl', idx_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file saved to: data/doc_level-sentiment/doc_level/data.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pickle(data_path, 'data.pkl', data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file saved to: data/doc_level-sentiment/doc_level/label.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pickle(data_path, 'label.pkl', label_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Doc_level_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
