{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practical_3.1 (TF 2).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNeLEVNw5yGgu6vDgUqCmRv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"L2iV47nQC8cc","colab_type":"text"},"source":["## Practical 3.1: word2vec\n","We recommend that you read the paper cited below before starting with this practical.\n","\n","**Reading material**\n","* [1] Mikolov, Tomas, et al. \"[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\" Advances in neural information processing systems. 2013. \n","\n","We start by loading the necessary libraries.\n"]},{"cell_type":"code","metadata":{"id":"BSp5SVoWGvxZ","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQ9ElevcC2S-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import keras.backend as K\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import plot_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kST6d2uDA8G","colab_type":"code","colab":{}},"source":["print(tf.__version__) #  check what version of TF is imported"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aH3XdAWzGcEY","colab_type":"text"},"source":["If you use Google Colab, you need to mount your Google Drive to the notebook when you want to use files that are located in your Google Drive. Paste the authorization code, from the new tab page that opens automatically when running the cell, in the cell below."]},{"cell_type":"code","metadata":{"id":"E9wiOf66GkFc","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MbvXUuPQIseK","colab_type":"text"},"source":["Navigate to the folder in which `alice.txt` is located. Make sure to start path with '/content/drive/My Drive/' if you want to load the file from your Google Drive."]},{"cell_type":"code","metadata":{"id":"tmERE-LmGm5y","colab_type":"code","colab":{}},"source":["cd '/content/drive/My Drive/Colab Notebooks/DL course/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpKpgHwAJCKa","colab_type":"code","colab":{}},"source":["# read the file \n","file_name = 'alice.txt'\n","corpus = open(file_name).readlines()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6EhDHdCKU--","colab_type":"code","colab":{}},"source":["corpus[:5]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e12RccsaJLuH","colab_type":"text"},"source":["## Data Preprocessing\n","\n","Before we can use the data in our SkipGram model, we need to convert each unique word to a unique numberic identifier. First we clean the data, where we remove punctuation from the text. Then a `Tokenizer` is fit on the cleaned corpus. This `Tokenizer` vectorizes a text corpus, by turning each sentence in the corpus into a sequence of integers."]},{"cell_type":"code","metadata":{"id":"AZ1HppHKI4Wr","colab_type":"code","colab":{}},"source":["# Removes sentences with fewer than 3 words\n","corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n","\n","# remove punctuation in text and fit tokenizer on entire corpus\n","tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n","tokenizer.fit_on_texts(corpus)\n","\n","# convert text to sequence of integer values\n","corpus = tokenizer.texts_to_sequences(corpus)\n","n_samples = sum(len(s) for s in corpus) # total number of words in the corpus\n","V = len(tokenizer.word_index) + 1 # total number of unique words in the corpus + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sWL5pWxKXBB","colab_type":"code","colab":{}},"source":["corpus[:1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYIZn1DqpGsK","colab_type":"text"},"source":["We add `1` to the number of unique words, such that we are also able to map  unknown words to one unique index value.  So, index `len(V)` corresponds to the unknown words that are not in the vocabulary, if you encouter those in future prediction tasks. We do not map unknown words to the index 0, because we want to be able to distinguish between a padded sequences (when you want to pad a sequence, you add zeros in front or at the end) and an unknown word that is not in the vocabulary. However, in this notebook we do not pad the sequence and we do not use words outside the vocabulary, so don't think about it too much for now."]},{"cell_type":"code","metadata":{"id":"IQwU_ernPbQl","colab_type":"code","colab":{}},"source":["n_samples, V"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYvhfx5Tuf_F","colab_type":"code","colab":{}},"source":["# example of how word to integer mapping looks like in the tokenizer\n","print(list((tokenizer.word_index.items()))[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEYKEUcsrIsA","colab_type":"code","colab":{}},"source":["print(list((tokenizer.word_index.items()))[-5:])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WDvvWgPpwgUT","colab_type":"text"},"source":["## Generate data for Skipgram\n","Skipgram predicts the source context words given a target word. We break down each (target word, context word**s**) pair into a **single** pair (target word, context word). This is what we do in the function `generate_data_skipgram`, which returns two NumPy arrays: `x` (input, i.e. target word) and `y` (output, i.e. context word)."]},{"cell_type":"code","metadata":{"id":"WDeUCy79L7P2","colab_type":"code","colab":{}},"source":["window_size = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCzI9nt63aXq","colab_type":"code","colab":{}},"source":["corpus[:1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4oty1h2MCC6","colab_type":"code","colab":{}},"source":["#generate data for Skipgram\n","def generate_data_skipgram(corpus, window_size, V):\n","    maxlen = window_size*2\n","    all_in = []\n","    all_out = []\n","    for words in corpus:\n","        L = len(words)\n","        for index, word in enumerate(words):\n","            p = index - window_size\n","            n = index + window_size + 1\n","                    \n","            in_words = []\n","            labels = []\n","            for i in range(p, n):\n","                if i != index and 0 <= i < L:\n","                    # Add the input word\n","                    #in_words.append(word)\n","                    all_in.append(word)\n","                    # Add one-hot of the context words\n","                    all_out.append(to_categorical(words[i], V))\n","                                      \n","    return (np.array(all_in),np.array(all_out))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nTPlCVNPsyy","colab_type":"code","colab":{}},"source":["x, y = generate_data_skipgram(corpus, window_size, V)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CHvRSxpw5Dc","colab_type":"code","colab":{}},"source":["x.shape, y.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZjuJciaMkZi","colab_type":"text"},"source":["## Create architecture Skipgram model\n","Implement the Skipgram model architecture. \n","\n","* Initialise a Keras Sequential model\n","* Add an Embedding layer. Think about the following parameter values: the input dimension (`input_dim`) , the size of the embedding vectors (`output_dim`) and the input length (`input_length`). Use the parameter `kernel_initializer` to initialize the weights with values from a `glorot_uniform` distribution. An embedding layer turns positive integers into a dense vector of size `dim`. This embedding layer learns a dense word embedding for all words in the vocabulary.\n","* Add a Reshape layer, which reshapes the output of the embedding layer (1,dim) to (dim,)\n","* Add a final Dense layer with the same size and activation function as in [1]. Use the parameter `kernel_initializer` to initialize the weights with values from a `glorot_uniform` distribution.\n","* Compile the model with a suitable loss function and select an optimizer."]},{"cell_type":"code","metadata":{"id":"XdEIR45TMHoi","colab_type":"code","colab":{}},"source":["dim = 100 # dimension of word embedding\n","\n","# TODO finish the model implementation\n","skipgram = ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QOQVVSwL5cOl","colab_type":"text"},"source":["The `?` in the output of `plot_model` corresponds to the batch size."]},{"cell_type":"code","metadata":{"id":"aPuL4w58x4H6","colab_type":"code","colab":{}},"source":["plot_model(skipgram, show_shapes = True, show_layer_names=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOIx9fFW2rjV","colab_type":"code","colab":{}},"source":["skipgram.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WC76oaL_57Tj","colab_type":"text"},"source":["<b>HINT</b>: To increase training speed of your model, you can use the free available GPU power in Google Colab. Go to `Edit` --> `Notebook Settings` --> select `GPU` under `hardware accelerator`."]},{"cell_type":"code","metadata":{"id":"HtQYxGhVMpo3","colab_type":"code","colab":{}},"source":["# TODO train the skipgram model\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5MWL2JbUytkH","colab_type":"text"},"source":["## Get word embeddings\n","The embedding matrix is saved in the weights of the model. "]},{"cell_type":"code","metadata":{"id":"CmnhFuzlS0LI","colab_type":"code","colab":{}},"source":["weights = skipgram.get_weights()\n","\n","print(\"Weights for the embedding layer: \",  weights[0].shape)\n","print(\"Weights for the dense layer: \",  weights[1].shape)\n","print(\"Biases for the dense layer: \",  weights[2].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbxy64MUS5zs","colab_type":"code","colab":{}},"source":["# Get the embedding matrix\n","embedding = weights[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJHCbTpLSU7s","colab_type":"code","colab":{}},"source":["# get word embeddings for each word in the vocabulary, write to file\n","\n","f = open('vectors_skipgram.txt' ,'w')\n","f.write(\" \".join([str(V-1),str(dim)]))\n","f.write(\"\\n\")\n","\n","for word, i in tokenizer.word_index.items():\n","    f.write(word)\n","    f.write(\" \")\n","    f.write(\" \".join(map(str, list(embedding[i,:]))))\n","    f.write(\"\\n\")\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4roJmc713h7T","colab_type":"text"},"source":["## Analogy function and performance comparison\n","\n","Now that the model is trained, we can get the word embedding for each word in the vocabulary. We can perform an analogy task (see [1] for concrete examples), by adding and subtracting the embedding of words from each other. In the code below we use the Euclidean distance metric to compute the distane between the predicted embedding and the true word embedding, however other distance metrices can also be used. \n","\n","To get the word embedding $e_{word}$, we take the dot product of the one-hot encoding of the word and the embedding matrix, i.e. $e_{word} = E_{embedding} \\cdot V_{one-hot-word}$ (see also the lecture notes)."]},{"cell_type":"code","metadata":{"id":"hR4SuPRbS6NU","colab_type":"code","colab":{}},"source":["# Embed a word by getting the one hot encoding and taking the dot product of this vector with the embedding matrix\n","# 'word' = string type\n","def embed(word, embedding=embedding, vocab_size = V, tokenizer=tokenizer):\n","    # get the index of the word from the tokenizer, i.e. convert the string to it's corresponding integer in the vocabulary\n","    int_word = tokenizer.texts_to_sequences([word])[0]\n","    # get the one-hot encoding of the word\n","    bin_word = to_categorical(int_word, V)\n","    return np.dot(bin_word, embedding)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uqcebiN2zHK4","colab_type":"text"},"source":["We can check whether an analogy function like $e_{king} - e_{queen} + e_{woman} \\approx e_{man}$ holds. In a perfect scenario, we would like that this formula ( $e_{king} - e_{queen} + e_{woman}$) results in the embedding of the word \"man\". However, it does not always result in exactly the same word embedding. The result of the formula is called the expected or the predicted word embedding. In this context, \"man\" is called the true or the actual word $t$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$. In this Practical we only compute the distance between the predicted embedding and the embedding of the true word. "]},{"cell_type":"code","metadata":{"id":"xyRoJlJCz9MB","colab_type":"code","colab":{}},"source":["# TODO: compute the embedding by implementing the formula above. Use the embed function to get an embedding of a word\n","predictedEmbedding = ... # expected embedding = e_man"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AHMTcrmzzc9Y","colab_type":"text"},"source":["Compute the distance between expectedEmbedding and the embedding of the true word `man`."]},{"cell_type":"code","metadata":{"id":"YE1Al0uezdKR","colab_type":"code","colab":{}},"source":["# TODO: compute distance between the predicted embedding and the true word embedding using the Euclidean distance metric\n","dist_exp_true = ...\n","print(dist_exp_true)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iayrcVxH1E9N","colab_type":"text"},"source":["We can use the distance metric to find if the expected word is equal to the actual word.  If the expected word embedding is the same as the word embedding of the true word, the model made a correct prediction."]}]}